{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of ARA Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline:**\n",
    "1. Read raw data\n",
    "2. Text preprocessing & cleaning\n",
    "3. Document representation (feature vectors)\n",
    "4. Build sentiment classifier based on BOW vectors\n",
    "5. Build sentiment classifier based on fasstext vectors\n",
    "\n",
    "**What you need to do:** \n",
    "- Read and execute the source code below and answer the questions in **EXERCISE 1 - EXERCISE 4**.\n",
    "- **Submit** the modifiled file ``TD3.ipynb`` on google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# set the font size of plots\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read corpus\n",
    "We are going to use a different corpus. This corpus is already labelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_files = [ './sentiment_data_ARA_pos.txt', './sentiment_data_ARA_neg.txt' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(filename):\n",
    "    print('Reading file ' + filename + \"...\")\n",
    "    with open(filename, \"r\", encoding='utf8') as textfile:\n",
    "        L = []\n",
    "        for line in textfile:\n",
    "            L.append(line.strip())\n",
    "        print('File contains ', len(L), \"lines.\\n\")\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file ./sentiment_data_ARA_pos.txt...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './sentiment_data_ARA_pos.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-06ca0845943d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mara_corpus_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_text_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mara_corpus_neg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_text_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-49b1960d4d0a>\u001b[0m in \u001b[0;36mread_text_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_text_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Reading file '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtextfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtextfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './sentiment_data_ARA_pos.txt'"
     ]
    }
   ],
   "source": [
    "ara_corpus_pos = read_text_file(corpus_files[0])\n",
    "ara_corpus_neg = read_text_file(corpus_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify corpus\n",
    "\n",
    "type(ara_corpus_pos),type(ara_corpus_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ara_corpus_pos),len(ara_corpus_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ara_corpus_pos[0])\n",
    "print(ara_corpus_neg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine pos and neg corpus into a single corpus for easy manipulation\n",
    "\n",
    "ara_corpus = ara_corpus_pos + ara_corpus_neg\n",
    "ara_corpus_sentiment = len(ara_corpus_pos)*[1] + len(ara_corpus_neg)*[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ara_corpus),len(ara_corpus_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing & Cleaning\n",
    "\n",
    "We are going to follow the same pipeline of TD1, except that here the operations should be **adapted to the Arabic language** (instead of the French language).\n",
    "\n",
    "1. Remove useless characters (using ``cleanup_text`` function from TD2)\n",
    "2. Language identification and filtering (using language identification model from TD2)\n",
    "3. Letter normalization\n",
    "4. Tokenization\n",
    "5. Remove stop words\n",
    "6. Word normalization (stemming)\n",
    "7. Remove words that are too short or too long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 1\n",
    "\n",
    "Apply the above text cleaning operations by completing the code below (follow the instructins in the comments).\n",
    "\n",
    "**However in order to save time, you will skip some steps in class and complete them at home.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPLETE THE SOURCE CODE IN THE CELLS BELOW. FOLLOW THE INSTRUCTIONS AND HINTS GIVEN IN THE COMMENTS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import unicodedata as ud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "stop_words_ar = stopwords.words('arabic')\n",
    "\n",
    "# regexp for word elongation: matches 3 or more repetitions of a word character.\n",
    "two_plus_letters_RE = re.compile(r\"(\\w)\\1{1,}\", re.DOTALL)\n",
    "three_plus_letters_RE = re.compile(r\"(\\w)\\1{2,}\", re.DOTALL)\n",
    "\n",
    "# regexp for repeated words\n",
    "two_plus_words_RE = re.compile(r\"(\\w+\\s+)\\1{1,}\", re.DOTALL)\n",
    "#arabic ponctuation\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "def cleanup_text(text):\n",
    "   \n",
    "    # Remove URLs\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text)\n",
    "\n",
    "    # Remove user mentions of the form @username\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "    \n",
    "    # Remove special useless characters such as _x000D_\n",
    "    text = re.sub(r'_[xX]000[dD]_', '', text)\n",
    "\n",
    "    # Remove redundant white spaces\n",
    "    text = text.strip()\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "\n",
    "    # normalize word elongations (characters repeated more than twice)\n",
    "    text = two_plus_letters_RE.sub(r\"\\1\\1\", text)\n",
    "\n",
    "    # remove repeated words\n",
    "    text = two_plus_words_RE.sub(r\"\\1\", text)\n",
    "\n",
    "    #remove repeating char\n",
    "    re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    \n",
    "    #remove diactritic\n",
    "    arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                             \"\"\", re.VERBOSE)\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    \n",
    "    #remove ponctuation\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    text=text.translate(translator)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply this function to each document in the corpus\n",
    "ara_corpus_clean = []\n",
    "for doc in ara_corpus:\n",
    "    ara_corpus_clean.append(cleanup_text(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مكان الرجل الحقيقي كل شي نضيف واصلي وماركات صح غالي شوي بس لبس روعه\n"
     ]
    }
   ],
   "source": [
    "assert(len(ara_corpus_clean)==len(ara_corpus))\n",
    "print(ara_corpus_clean[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##2. Language identification and filtering\n",
    "# You should just the machine-learning based model that you constructed in TD2 (load it from a file)\n",
    "\n",
    "# MODIFY THIS STEP AT HOME AS EXPLAINED ABOVE.\n",
    "\n",
    "# Remove the code below later\n",
    "# Quick method to save time in class: remove documents that contain more than 30% latin characters\n",
    "MAX_LAT_FRAC = 0.3\n",
    "ara_corpus_clean = [doc for doc in ara_corpus_clean if (len(re.findall('[a-zA-Z]',doc)) / len(doc)) < MAX_LAT_FRAC]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10898 10773\n",
      "مكان الرجل الحقيقي كل شي نضيف واصلي وماركات صح غالي شوي بس لبس روعه\n"
     ]
    }
   ],
   "source": [
    "assert(len(ara_corpus_clean)<=len(ara_corpus))\n",
    "print(len(ara_corpus),len(ara_corpus_clean))\n",
    "print(ara_corpus_clean[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##3. Letter normalization\n",
    "# Hint: which Arabic letters are equivalent n social media text?  e.g. alef, tah marbuta, dhad and dhad toushel, etc.\n",
    "\n",
    "# SKIP THIS STEP IN CLASS. COMPLETE IT AT HOME.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['مكان', 'الرجل', 'الحقيقي', 'كل', 'شي', 'نضيف', 'واصلي', 'وماركات', 'صح', 'غالي', 'شوي', 'بس', 'لبس', 'روعه'], ['جيد', 'جدا', 'المكان', 'يتمتع', 'بطاقم', 'ضيافة', 'علي', 'مستوي', 'عالي', 'منذ', 'دخولك', 'وهم', 'يستقبلوك', 'بالابتسامة', 'يتمتع', 'بموسيقي', 'هادئة', 'ولكن', 'ذلك', 'عندما', 'يكون', 'المطعم', 'غير', 'ممتليء', 'ولكني', 'لاحظت', 'العكس', 'عند', 'امتلاؤه', 'واتوقع', 'ان', 'هذا', 'لكي', 'يجعل', 'الزبائن', 'تأكل', 'بسرعة', 'اكبر', 'لكي', 'يتوافر', 'اماكن', 'اخري', 'لغيرهم', 'بالنسبة', 'للاكل', 'فهو', 'له', 'مذاق', 'جميل', 'وطيب', 'من', 'اللحطة', 'الاولي', 'تشعر', 'بان', 'كمية', 'الاكل', 'غير', 'كافية', 'وغير', 'مناسبة', 'للسعر', 'ولكن', 'عند', 'بدايتك', 'في', 'الاكل', 'تشعر', 'بجودته', 'وتشعر', 'بان', 'الكمية', 'معقولة', 'فعلا', 'الجلسات', 'مريحة', 'جدا', 'فيه', 'واسعاره', 'مناسبة', 'بالنسبة', 'لجودة', 'الاكل'], ['فندق', 'رائع', 'في', 'الواقع', 'الفندق', 'رائع', 'وكذلك', 'العاملين', 'أروع', 'فيه', 'من', 'ناحية', 'التعامل', 'والخدمة', 'الغرف', 'نظيفة', 'ولكنها', 'ضيقة', 'نوعا', 'ما', 'ولكن', 'هذه', 'فنادق', 'نيويورك', 'عجبني', 'ايضا', 'الفطور', 'الصباحي', 'الموقع', 'الرائع', 'للفندق', 'القريب', 'من', 'بقالة', 'والتايمز', 'سكوير', 'والفيفث', 'افنيو'], ['جميل', 'فندق', 'جميل', 'جدا', 'استمتعت', 'بالاقامه', 'فيه', 'كنت', 'هناك', 'من', 'اجل', 'التسوق', 'وقد', 'ساعدنى', 'موظفى', 'الاستقبال', 'فى', 'ذلك', 'كما', 'ان', 'الغرف', 'كانت', 'مريحه', 'جدا', 'واسلوب', 'الضيافه', 'كان', 'رائعا', 'الكل', 'كانوا', 'لطاف', 'معى', 'لم', 'يعطوننى', 'احساس', 'اننى', 'بعيدا', 'عن', 'وطنى'], ['ممتاز', 'بكل', 'المقاييس'], ['جميلة', 'وعملية', 'وسهلة', 'الاستخدام', 'والحمل', 'والتخزين', 'ملائمة', 'للاستعمال', 'الشخصى', 'والمنزلى', 'لا', 'تحتاج', 'الى', 'احترافية', 'فى', 'الاستخدام', 'تعطى', 'تموجات', 'حلزونية', 'جميلة'], ['السعر', 'معقول', 'و', 'العطر', 'طيب', 'جدا'], ['جيد', 'جدا', 'مطعم', 'يهتم', 'بالصحه', 'والاكل', 'نظيف', 'خالي', 'من', 'الدهون', 'اغلب', 'الرواد', 'من', 'المهتمين', 'بصحتهم', 'والمهتمين', 'بالاكل', 'الصحي', 'لايقدم', 'المشروبات', 'الكحوليه'], ['فندق', 'رائع', 'وفخم', 'من', 'اجمل', 'فنادق', 'القاهرة', 'فندق', 'رائع', 'وفخم', 'من', 'اجمل', 'فنادق', 'القاهرة', 'اقمت', 'في', 'الفندق', 'اخر', 'شهر', 'فبراير', '2013', 'غرف', 'فخمه', 'ورائعه', 'من', 'جميع', 'النواحي', 'حمام', 'بتنسيق', 'جميل', 'جدا', 'اعجبني', 'جدا', 'الجاكوزي', 'وحمامات', 'السباحه', 'وجميع', 'خدمات', 'السبا', 'فريق', 'العمل', 'كان', 'ودود', 'جدا'], ['Le', 'Lido', 'من', 'افضل', 'العروض', 'الترفيهية', 'في', 'باريس', 'يستحق', 'الزيارة', 'وسعره', 'مناسب', 'ولكن', 'لا', 'يصلح', 'للأطفال', 'السعر', 'يشمل', 'العشاء', 'مع', 'مشروب', 'وبالإستطاعة', 'الحجز', 'عن', 'طريق', 'الموقع', 'الإلكتروني', 'والزي', 'المناسب', 'لدخول', 'المكان', 'هو', 'الزي', 'الشبه', 'رسمي', 'قميص', 'وبنطلون'], ['فندق', 'ممتاز', 'أقمت', 'في', 'هذا', 'الفندق', 'خلال', 'الفترة', 'من', '12', 'إلى', '15', 'نوفمبر', '2014', 'في', 'رحلة', 'عمل', 'تحديد', 'ا', 'في', 'غرفة', 'كريستال', 'كلوب', 'كان', 'كل', 'شيء', 'مثالي', 'ا', 'الغرفة', 'كانت', 'نظيفة', 'وفسيحة', 'وتوافرت', 'لي', 'إمكانية', 'استخدام', 'نادي', 'رجال', 'الأعمال', 'في', 'الطابق', 'العاشر', 'لأخذ', 'قسط', 'من', 'الترفيه', 'حيث', 'المقب', 'لات', 'والمأكولات', 'والقهوة', 'والمشروبات', 'طوال', 'اليوم', 'كان', 'المكان', 'جيد', 'ا', 'لمقابلة', 'عملاء', 'الشركة', 'يضم', 'الفندق', 'أيض', 'ا', 'غرفة', 'مؤتمرات', 'متوسطة', 'المساحة', 'وجميع', 'هذه', 'المرافق', 'مجانية', 'وكانت', 'الغرفة', 'متألقة', 'ونظيفة', 'ومزودة', 'بالقهوة', 'والشاي', 'ومياه', 'الشرب', 'أما', 'الموظفون', 'فكانوا', 'لطفاء', 'ومتعاونين', 'سوف', 'أقيم', 'في', 'هذا', 'الفندق', 'مجدد', 'ا', 'في', 'رحلة', 'عملي', 'التالية'], ['جهاز', 'دقيق', 'في', 'النتائج', 'وبامكانك', 'اختيار', 'نوعية', 'القراءة', 'مرة', 'واحدة', 'أو', 'ثلاث', 'مرات', 'ويعطيك', 'المتوسط', 'الحسابي', 'للقراءات'], ['معلم', 'سياحي', 'جميل', 'عندما', 'تنظر', 'للسور', 'وتعلم', 'ان', 'هذا', 'المعلم', 'قائم', 'من', 'ايام', 'البيزنطيين', 'تدهش', 'لحجمة', 'الكبير', 'وبناء', 'الرائع'], ['على', 'أعلى', 'مستوى', 'وغير', 'باهظ', 'وصلنا', 'الساعة', '4', '00', 'صباح', 'ا', 'من', 'إسطنبول', 'وظللنا', 'نائمين', 'حتى', 'الساعة', '11', '30', 'صباح', 'ا', 'تم', 'تجهيز', 'الإفطار', 'بالرغم', 'من', 'انتهاء', 'الفترة', 'التي', 'يتم', 'تقديمه', 'فيها', 'فريق', 'العمل', 'كان', 'مرحب', 'متفهم', 'ومحترف', 'يستحق', 'أعلي', 'الدرجات', 'على', 'الغرف', 'الجميلة', 'بالفعل', 'والتي', 'تم', 'تصميمها', 'للأشخاص', 'الحقيقيين', 'وللاحتياجات', 'الفعلية', 'للمسافرين', 'من', 'أجل', 'العمل', 'يوجد', 'بالحمام', 'دش', 'ساخن', 'رائع', 'الإضاءة', 'وتوصيلات', 'الكهرباء', 'الخاصة', 'بطاولة', 'العمل', 'ممتازة', 'تستحق', 'أعلى', 'الدرجات'], ['فندق', 'صغير', 'رائع', 'أقمنا', 'هنا', 'لمدة', 'ثلاث', 'ليال', 'في', 'أغسطس', 'وأذهلنا', 'الترحيب', 'الودود', 'وكيف', 'ظهرنا', 'استثنائيين', 'وقد', 'اعتنى', 'بنا', 'المالكون', 'وطاقم', 'العاملين', 'بالفندق', 'الموقع', 'رائع', 'كذلك', 'التماثيل', 'العملاقة', 'على', 'بعد', 'مسافة', 'سير', 'يسيرة', 'وكان', 'لطيف', 'ا', 'أن', 'أعود', 'للغرف', 'النظيفة', 'والمريحة', 'بعد', 'يوم', 'من', 'الاستكشاف', 'فإذا', 'عدنا', 'مجدد', 'ا', 'سنزوره', 'مرة', 'أخرى'], ['فندق', 'جيد', 'بسعر', 'مناسب', 'فندق', 'متميز', 'بسعر', 'مناسب', 'الغرف', 'نظيفة', 'و', 'طاقم', 'الخدمة', 'ودود', 'و', 'متعاون'], ['تجربة', 'لطيفة', 'للغاية', 'لقد', 'قمنا', 'بالحجز', 'من', 'الإنترنت', 'لمعرض', 'آي', 'تي', 'بي', 'برلين', 'وعندما', 'رأيت', 'الصور', 'لم', 'أكن', 'متأكد', 'ا', 'مما', 'إذا', 'كان', 'الفندق', 'أعجبني', 'أم', 'لا', 'لكني', 'عندما', 'وصلنا', 'إلى', 'الفندق', 'عرفنا', 'مع', 'أصدقائي', 'أن', 'هذا', 'كان', 'هو', 'الخيار', 'الصحيح', 'المبنى', 'قديم', 'جد', 'ا', 'وجميل', 'مثل', 'كل', 'برلين', 'لكنه', 'واسع', 'جد', 'ا', 'وبه', 'غرف', 'مريحة', 'وموقع', 'جيد', 'جد', 'ا', 'وأقدم', 'شكري', 'الخاص', 'لكل', 'موظفي', 'الاستقبال', 'كلهم', 'كانوا', 'أناس', 'ا', 'رائعين', 'وبشوشين', 'ومتعاونين', 'وودودين', 'جد', 'ا', 'وأنا', 'أوصي', 'بالتأكيد', 'بهذا', 'الفندق'], ['متعة', 'فعلا', 'لاتشعر', 'بالطريق', 'من', 'المتعة', 'والمناظر', 'الطبيعية', 'الخلابة', 'حولك', 'النمسا', 'كلها', 'عموما', 'رائعة', 'وطرقها', 'مريحة', 'وكل', 'من', 'يسافر', 'فيها', 'يتمنى', 'أن', 'الوقت', 'يطول', 'عكس', 'الطرق', 'في', 'أي', 'بلد', 'آخر'], ['مكان', 'رائع', 'مع', 'الأطفال', 'مكثنا', 'لليلة', 'واحدة', 'مع', 'الأطفال', 'الغرف', 'سعتها', 'جيدة', 'غرفتنا', 'كانت', 'نظيفة', 'و', 'مطلة', 'على', 'حديقة', 'الحيوانات', 'حديقة', 'الحيوانات', 'جيدة', 'جدا', 'ليست', 'كبيرة', 'جدا', 'خاصة', 'اذا', 'كان', 'لديك', 'أطفال', 'صغار', 'أطفالي', 'استمتعوا', 'كثيرا', 'بتجربة', 'اطعام', 'الحيوانات', 'باليد', 'الغرف', 'مجهزة', 'بمطبخ', 'صغير', 'لا', 'توجد', 'طاهية', 'مع', 'بالوعة', 'ثلاجة', 'صغيرة', 'و', 'خزائن', 'حائط'], ['الاهرمات', 'یوجد', 'بها', 'احدی', 'عجاب', 'الدنیا', 'السبع', 'وفخرل', 'مصری', 'انها', 'من', 'اگثر', 'الاماگن', 'التی', 'یوجد', 'بها', 'اثار', 'صامدا', 'فی', 'وجه', 'الدهر'], ['مدينة', 'العاب', 'لها', 'طابع', 'خاص', 'زرنا', 'المدينة', 'لمدة', 'يوم', 'لكن', 'المشكلة', 'مايكفي', 'لها', 'يوم', 'مدينة', 'متنوعه', 'بقطارات', 'الموت', 'والعاب', 'الاطفال', 'مدينة', 'جمعت', 'جميع', 'الفئات', 'العمرية', 'كانت', 'زيارتنا', 'لها', 'بالسيارة', 'من', 'سلوفاكيا', 'الى', 'فيينا', 'ثم', 'ميونخ', 'ثم', 'راست', 'مدينة', 'الالعاب'], ['مجمع', 'يوفر', 'كل', 'شئ', 'من', 'اجهزة', 'الكترونية', 'و', 'لوازم', 'منزلية', 'و', 'العاب', 'و', 'ملابس', 'و', 'هو', 'متعة', 'العائلة', 'انصح', 'زيارته', 'للتمعة', 'الكبيرة', 'الخدمات', 'التي', 'يوفرها'], ['هذا', 'ما', 'كان', 'ينتظره', 'اليمنييون', 'ولا', 'زلنا', 'نتعطش', 'للمزيد', 'والمزيد', 'في', 'ظل', 'قيادتكم', 'الحكيمة', 'اصبحنا', 'اليمنيينون', 'في', 'المهجر', 'نتوق', 'الى', 'بناء', 'بلدنا', 'بايدينا'], ['فندق', 'رائع', 'لقد', 'استمتعت', 'في', 'فندق', 'الشعله', 'بالدوحه', 'انه', 'فعلا', 'فندق', 'رائع', 'وجميل', 'من', 'افضل', 'الاماكن', 'هناك', 'المطعم', 'الموجود', 'في', 'الطابق', '47', 'المنظر', 'مريح', 'للغايه', 'يمكنك', 'ان', 'تري', 'الدوحه', 'بالكامل', 'خلال', 'وجبه', 'العشاء', 'انصح', 'الجميع', 'من', 'السعوديه', 'بهذه', 'التجربه', 'لدي', 'تعليق', 'واحد', 'وهو', 'ليس', 'هناك', 'قنوات', 'تليفيزيون', 'للاطفال'], ['قبل', '22', 'يوم', 'المطعم', 'المفضل', 'لي', 'لأسباب', 'كثيرة', 'منهـا', '1', 'روعة', 'التصميم', 'التراثي', 'الذي', 'بالتأكيد', 'يأسرني', 'في', 'كل', 'مرة', '2', 'جودة', 'الأكل', 'وناحية', 'النظافة', 'لدى', 'العمـالة', '3', 'يذكرني', 'كثيرا', 'بطبخ', 'البيت', 'وهنا', 'الاصناف', 'التي', 'جربتها', 'في', 'المطعم', 'وجميعهـا', 'رائعة', '1', 'سلطة', 'حارة', 'سلطة', 'بصل', 'سلطة', 'خضار', '2', 'شوربة', 'جريش', 'شوربة', 'ضلوع', 'لحم', 'مرقة', 'خضار', '3', 'مقلقل', 'لحم', 'حاشي', 'غنم', 'حميس', '4', 'كبسة', 'لحم', 'حاشي', 'كبسة', 'لحم', 'غنم', 'كبسة', 'دجاج', '5', 'بادية', 'حاشي', 'بادية', 'غنم', 'بادية', 'دجاج', 'للتوضيح', 'البادية', 'عبارة', 'عن', 'رز', 'قرصان', 'جريش', '6', 'مقلوبة', 'دجاج', 'مقلوبة', 'لحم', '7', 'مضغوط', 'دجاج', 'مضغوط', 'لحم', '8', 'جريش', 'سليق', 'قرصان', 'مرقوق', 'مطازيز', 'باقي', 'الأصناف', 'ما', 'جربتهــا', 'إلا', 'الآن', 'انصح', 'الجميع', 'بزيارته', 'راائع', 'للغاية']]\n"
     ]
    }
   ],
   "source": [
    "##4. Tokenization -- Complete code below (same as in TD1)\n",
    "\n",
    "# COMPLETE THE CODE BELOW\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer('[^_\\W]+')\n",
    "ara_corpus_tokenized = [tokenizer.tokenize(doc) for doc in ara_corpus_clean]\n",
    "print(ara_corpus_tokenized[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify corpus after tokenization\n",
    "assert(len(ara_corpus_clean) == len(ara_corpus_tokenized))\n",
    "assert(type(ara_corpus_tokenized[0]) == list and type(ara_corpus_tokenized[0][0]) ==str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['مكان',\n",
       " 'الرجل',\n",
       " 'الحقيقي',\n",
       " 'كل',\n",
       " 'شي',\n",
       " 'نضيف',\n",
       " 'واصلي',\n",
       " 'وماركات',\n",
       " 'صح',\n",
       " 'غالي',\n",
       " 'شوي',\n",
       " 'بس',\n",
       " 'لبس',\n",
       " 'روعه']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify first document in corpus\n",
    "ara_corpus_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Monta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي']\n",
      "10773\n"
     ]
    }
   ],
   "source": [
    "##5. Remove stop words -- based on a 'standard' list of stopwords for the Arabic language.\n",
    "\n",
    "# COMPLETE THE CODE BELOW  (See TD1)\n",
    "\n",
    "# Load stop words from NLTK library\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words_ar = stopwords.words('arabic')\n",
    "type(stop_words_ar),len(stop_words_ar)\n",
    "print(stop_words_ar[0:10])\n",
    "\n",
    "# FEEL FREE TO ADD MORE WORDS TO THIS LIST IF YOU WANT ...\n",
    "\n",
    "\n",
    "# For each document, remove stop words\n",
    "ara_corpus_tokenized = [[word for word in doc  if word not in stop_words_ar] for doc in ara_corpus_tokenized]\n",
    "print(len(ara_corpus_tokenized))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify corpus after removing stop words\n",
    "assert(len(ara_corpus_clean) == len(ara_corpus_tokenized))\n",
    "assert(type(ara_corpus_tokenized[0]) == list and type(ara_corpus_tokenized[0][0]) ==str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['مكان',\n",
       " 'الرجل',\n",
       " 'الحقيقي',\n",
       " 'شي',\n",
       " 'نضيف',\n",
       " 'واصلي',\n",
       " 'وماركات',\n",
       " 'صح',\n",
       " 'غالي',\n",
       " 'شوي',\n",
       " 'لبس',\n",
       " 'روعه']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify first document\n",
    "ara_corpus_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['مكان', 'رجل', 'حقيق', 'شي', 'نضيف', 'واصل', 'ماركا', 'صح', 'غال', 'شو', 'لبس', 'روع']\n",
      "10773\n"
     ]
    }
   ],
   "source": [
    "##6. Stemming\n",
    "# Hints: stemming is a difficult task for the Arabic language because words are often combined into one word (called agglutination).\n",
    "#     You should first visually inspect all the words in your corpus to get an idea about which words are good candidates for stemming ...\n",
    "#     Then try to think of a few simple stemming heuristics (regular expressions), such as: remove certain prefixes (e.g. al), remove certain suffixes (e.g. 'ouna') ...\n",
    "# SKIP THIS STEP IN CLASS TO SAVE TIME. COMPLETE IT AT HOME.\n",
    "from snowballstemmer import stemmer\n",
    "ara_corpus_doc= []\n",
    "ara_corpus_stemmed=[]\n",
    "ar_stemmer = stemmer(\"arabic\")\n",
    "for doc in ara_corpus_tokenized:\n",
    "    for word in doc:\n",
    "        ara_corpus_doc.append(ar_stemmer.stemWord(word))\n",
    "    \n",
    "    ara_corpus_stemmed.append(ara_corpus_doc)\n",
    "    ara_corpus_doc= []\n",
    "print(ara_corpus_stemmed[0])\n",
    "print(len(ara_corpus_stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##7. Remove words that are too short or too long.\n",
    "# Very short words are usually not very meaningful.\n",
    "# Very long words are usually either wrong, or elongated, or derived words that should be stemmed (normalized).\n",
    "\n",
    "# ENTER YOUR CODE BELOW. See TD1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ara_corpus_stemmed = [[word for word in doc if len(word)>=2] for doc in ara_corpus_stemmed]\n",
    "\n",
    "\n",
    "#this stemming is not very helpfull in our case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Representation\n",
    "Remember that the goal of this step is to transform each document from a list of words to a numeric *feature vector*. Each feature corresponds to a distinct word in a selected vocabulary. These feature vectors are stored in a numeric matrix called the **Document-Term Matrix (DTM)**.\n",
    "\n",
    "We will achieve this using *tfidf-BOW* method, as we did in **TD1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE THE CODE BELOW THEN ANSWER THE QUESTIONS IN THE EXERCISE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the corpus for BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, concatenate the words in the cleaned corpus (because BOW method in scikit-learn requires this format)\n",
    "ara_corpus_bow = [' '.join(doc) for doc in ara_corpus_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(ara_corpus_stemmed)==len(ara_corpus_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مكان رجل حقيق شي نضيف واصل ماركا صح غال شو لبس روع\n"
     ]
    }
   ],
   "source": [
    "print(ara_corpus_bow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the vocabulary set\n",
    "Extract the vocabulary set from our corpus and calculate IDF values of each word in this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters of the BOW model\n",
    "# FEEL FREE TO MODIFY THESE PARAMETERS AS NEEDED ...\n",
    "max_words = 10000\n",
    "maxdf = 1.0\n",
    "mindf = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of this class\n",
    "bow_model = TfidfVectorizer(max_df=maxdf, min_df=mindf, max_features=max_words, stop_words=[], use_idf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=10000, min_df=0.01,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=[], strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call fit() method in order to prepare BOW method (determine vocabulary and IDF values)\n",
    "bow_model.fit(ara_corpus_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the DTM matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the transform method in order to calculate DTM matrix of our corpus\n",
    "ara_bow_dtm = bow_model.transform(ara_corpus_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(10773, 767)\n"
     ]
    }
   ],
   "source": [
    "# Verify the type and size of this matrix\n",
    "print(type(ara_bow_dtm))\n",
    "print(ara_bow_dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visually inspect the vocabulary\n",
    "This should help you **tune** the BOW configuration parameters (i.e. min_df, max_df, etc.) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 767)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The vocabulary of BOW -- i.e. the words that were selected by BOW method to be in the vocabulary\n",
    "bow_vocab = bow_model.get_feature_names()\n",
    "type(bow_vocab), len(bow_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set, 29298)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The words that were ignored (and were not included in the vocabulary)\n",
    "ignored_words = bow_model.stop_words_\n",
    "type(ignored_words),len(ignored_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ممتازللاقام', 'واتفرج', 'اصول', 'sim', 'تجاذب', 'ياصاحب', 'يكسب', 'زوق', 'موير', 'تقطت', 'عيوب', 'كلن', 'كباق', 'تتخربط', 'لاري', 'انخلع', 'متعلقات', 'يستعمل', 'شنو', 'رواعه', 'بالقو', 'معوقا', 'يشاور', 'ششهر', 'اطيل', 'تسود', 'اغذ', 'مننتجع', 'الجيش', 'لتهاب', 'عصر', 'فصل', 'يقتر', 'جوتل', 'سيدي', 'اضرب', 'كنصب', 'نتوج', 'خاصةحصل', 'معتبر', 'اسعا', 'رضع', 'هاديه', 'ذمه', 'ابدل', 'تانلستيل', 'يمشي', '225', 'سالت', 'يبرق', 'انيمش', 'المش', 'مسوال', 'جاكوب', 'ميريتوس', 'حذرني', 'ازصي', 'يرووح', 'كثاف', 'يجرء', 'حنرجع', 'ايس', 'اثق', 'دلت', 'اادر', 'توكابتك', 'مغلقه', 'اتوب', 'اوملي', 'شاق', 'والمس', 'بانا', 'يدار', 'كوزم', 'وتوغراف', 'ورلد', 'ممتيز', 'متحمس', 'besarabsky', 'عشاان', 'ظاهرا', 'فندقيه', 'ندوق', 'تشويق', 'ماتلاقي', 'اتجد', 'صلي', 'اندا', 'غلط', 'الجص', 'بلح', 'ابحر', 'استنشاق', 'اوعد', 'تعود', 'لانجر', 'ب38', 'مجمد', 'كقيم', 'شفط', 'ديجول', 'نتظر', 'والكو', 'ماخوذ', 'امبروسي', 'كليمار', 'يبار', 'ايظ', 'اصطحاب', 'يذهل', 'ينتيد', 'لبناني', 'مستور', 'سجلا', 'عادا', 'سمن', 'تلفز', 'تتش', 'تود', 'دكان', 'ثكن', 'ازدواج', 'اسطمبول', 'انتسغرام', 'ضلل', 'ايريك', 'تتحل', 'يتفاجء', 'سرفيس', 'تختبر', 'تقال', 'شناوي', 'قتال', 'ويل', 'فرجه', 'يوقف', 'سدد', 'park', 'والس', 'ابهيشيخ', 'كتشب', 'خناق', 'والب', 'يقو', 'جمميلل', 'يتاهل', 'كورد', 'اتجاوز', 'ارت', 'همهم', 'ياباني', 'دينيه', 'باسم', 'حصين', 'رونق', 'شاورل', 'يقدر', 'جدناس', 'انتظارف', 'نزكر', 'كده', 'نوافق', 'قوت', 'ياس', 'موعد', 'نطمء', 'ملتف', 'ذبح', 'معبد', 'وجاها', 'بتسام', 'طوييل', 'يلووع', 'طاج', 'باستثناء', 'وابق', 'هستير', 'باستمتاع', 'حقيب', 'خايف', 'باستصغار', 'يد', 'استجمام', 'راون', 'كابينيه', 'ينبع', 'ياللسخف', 'ابوتشين', 'شروط', 'انب', '28جن', '20014', 'حوضين', 'of', 'مدت', 'مزاح', 'يلخبط', 'شاوينج', 'اني', 'ياضا', 'غزي', '2013الي', 'سكارفا', 'جبس', 'عمبتاكل', 'تتوقف', 'مقيت', 'متبقي', 'yee', 'يفوت', 'لطخ', 'تصاعد', 'احتجاز', 'جلاليب', 'اري', 'احلی', 'ديكورا', 'عنايي', 'لاء', 'customer', 'غراهام', 'عبدرب', 'تساقط', 'عزت', 'اتعرض', 'مبارح', 'تتمثل', 'خربول', 'معابد', 'ماذل', 'اترب', 'دقا', 'يتوجه', 'يقام', 'ترفيهيه', 'ننكر', 'اسواالاستف', 'رووف', 'اوكر', 'حضرم', 'طورير', 'حتشبسو', 'قنع', 'يمني', 'تتاسف', 'ناسف', 'عملاق', 'مادري', 'تالم', 'مقامر', 'نتغد', 'اربد', 'نومت', 'واخماد', 'غذاييه', 'نعيب', 'تو', 'rayhaan', 'بلدم', 'مراقب', 'والمح', 'تعايش', 'احتاياجا', 'غافل', 'الاغذ', 'اعترف', 'موسوع', 'لانش', 'بانهيار', 'ومي', 'ارجاء', 'see', 'اسواق', 'اسيب', 'والرء', 'اثري', 'قفز', 'لفايف', 'دايمه', 'افتكر', 'واطعم', 'مذقا', 'مستحضرا', 'طرازفنادق', 'اكتوبر', 'خبث', 'فوول', 'مغفره', 'حدوديه', 'يتمثل', 'براج', 'يامتلحقش', 'نخشي', 'سىد', 'يحكول', 'تجمع', 'يخير', 'يخدش', 'يكوجد', 'نعزم', 'هيتعدل', 'منثور', 'تبن', 'متبع', 'محافظ', 'وذل', 'اصلاحا', 'عنيف', '21يوم', 'جيمز', 'يامشير', 'مني', 'ومبارد', 'sinaway', 'القص', 'مرتد', 'مابعجب', '280', 'وانصب', 'حقوق', 'استتاف', 'دعيل', 'international', 'نختلف', 'ممتازحت', 'انكورتار', '69', 'موجها', 'ماتكل', 'نترول', '1429لم', 'وكتيل', 'يدع', 'لضيف', 'ءاخير', 'جواهر', 'وسن', 'يحفض', 'عوجاء', 'اعشق', 'احبط', 'لربع', 'اسكريم', 'تحريك', 'باله', 'واللذ', 'اتيزي', 'متجانس', 'ينعم', 'محضر', 'اثيف', 'واتباع', 'وبا', 'معسول', 'نتعب', 'مستشفيا', 'باربيك', 'دهل', '22', 'تسعير', 'راو', 'واسسعار', 'فشار', 'اغوص', 'احتساء', '309', 'ارز', 'تاسس', 'فاستغل', 'elbatin', 'تراستيفير', 'منص', 'ختصار', 'تحذر', 'متناوب', 'اميك', 'لقلب', 'بساط', 'قيمت', 'يفرط', 'سامسونج', 'مشان', 'جبات', 'مليو', 'رحلاتي', 'ملذا', 'مديينت', 'ومك', 'سوت', 'وميتال', 'بيرر', 'نحمل', 'ملاصقه', 'قطعه', 'تقتنع', 'ساراسوات', 'وانظاف', 'قبول', '636', 'المطعم', 'االفجر', 'فاجع', 'اومليت', 'جبه', 'مدربا', 'متننمق', 'سمرلاند', 'يبع', 'واسعارد', 'تتسم', 'امومه', 'better', 'قداس', 'يشتر', 'معقد', 'بايفعل', 'منته', 'رتقال', 'صحبات', 'اختطاف', 'واختيارا', 'يزداد', 'ابانوب', 'ماترهور', 'زوء', 'ايت', 'وافترض', 'يفتش', 'غلرف', 'الوم', 'مضيء', 'moulim', 'قزر', 'مصطلح', 'اتظهر', 'تاون', 'يبتسم', 'دوانتس', 'يناري', 'مزود', 'شموع', 'ثدي', 'واحتيال', 'غطايا', 'امميز', 'ظراف', 'رامج', 'لشرم', '9حتي20', 'شريك', 'تحصل', 'iraq', 'انازر', 'كتيب', 'قوج', 'توبكابي', 'واحقر', 'عاز', 'راط', 'يزرو', 'يلامس', 'واليس', 'اجتماعيه', 'عاوز', 'كابه', 'اصطياف', 'تحيات', 'تهنء', 'ترو', 'فكن', 'انترلوك', 'يكس', 'يقمون', 'دخيل', 'activator', 'كناف', 'خايب', 'واستجمام', 'احمق', 'لاجر', 'مناظرالت', 'لهول', 'كراش', 'ناسو', 'نس', 'تعزم', 'ينتو', 'سجاير', '110', 'تداخل', 'ميرا', 'جمام', 'اشنع', 'مطف', 'بالاف', 'تنسون', 'يجاور', 'جربت', 'تتبيل', 'تشوفش', 'ونتيننتال', '717', 'زبرجد', 'حلوء', 'متقلب', 'متذمر', 'رعد', 'ملحوط', 'بالشا', 'مصيب', 'ينام', 'استورد', 'مقيته', 'غربه', 'سحل', 'فروع', 'مصايف', 'مكافح', 'اناكم', 'عجو', '3500', 'بطن', 'ويترز', 'تغميس', 'تنسحب', 'ورنرز', 'يخنق', 'سمت', 'وانار', 'مرووع', 'ضياع', 'بالحد', 'عصاير', 'هىالمواصفا', 'جاو', 'واوقح', '16', 'متضمننء', 'لقب', 'رووعه', 'قارص', 'نادل', 'امنن', 'بابتسام', 'صلو', 'حار', 'شوفل', 'هاولاء', 'قماش', 'هناكمك', 'وحبه', 'تاي', 'جليط', 'محيسني', 'اسبير', 'سكوت', 'هسال', 'مزحوم', 'متلهف', 'جيري', 'طالبا', '365', '76', 'نبه', 'نتحقق', 'ايش', '93', 'جلاب', 'ويج', 'وكال', 'سنوص', 'مييزه', 'فااشل', 'لبنين', 'مشاء', 'انترلاك', 'عوج', 'دهر',\n"
     ]
    }
   ],
   "source": [
    "#### DON'T DO THIS !!! THERE ARE TOO MANY IGNORED WORDS\n",
    "print(str(ignored_words)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put vocavulary and their IDF values in a data frame\n",
    "df = pd.DataFrame(dict(Word=bow_vocab,IDF=bow_model.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.336856</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.514206</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.505768</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.329064</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.172903</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        IDF Word\n",
       "0  4.336856   10\n",
       "1  5.514206  100\n",
       "2  5.505768   11\n",
       "3  5.329064   12\n",
       "4  5.172903   15"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>3.168497</td>\n",
       "      <td>يوجد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>5.522717</td>\n",
       "      <td>يورو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>5.593543</td>\n",
       "      <td>يوفر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>3.225273</td>\n",
       "      <td>يوم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>5.514206</td>\n",
       "      <td>يومين</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF   Word\n",
       "762  3.168497   يوجد\n",
       "763  5.522717   يورو\n",
       "764  5.593543   يوفر\n",
       "765  3.225273    يوم\n",
       "766  5.514206  يومين"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          10\n",
       "1         100\n",
       "2          11\n",
       "3          12\n",
       "4          15\n",
       "5          20\n",
       "6        2012\n",
       "7          30\n",
       "8          50\n",
       "9         ابد\n",
       "10      اتصال\n",
       "11       اتصل\n",
       "12      اتمني\n",
       "13       اثاث\n",
       "14      اثناء\n",
       "15       اجاز\n",
       "16        اجد\n",
       "17        اجر\n",
       "18        اجل\n",
       "19       اجمل\n",
       "20        احب\n",
       "21       احبب\n",
       "22     احترام\n",
       "23        احد\n",
       "24        احس\n",
       "25       احمد\n",
       "26       اخبر\n",
       "27     اختيار\n",
       "28        اخذ\n",
       "29        اخر\n",
       "        ...  \n",
       "737       ويم\n",
       "738      يارا\n",
       "739      يبدو\n",
       "740      يبعد\n",
       "741       يتم\n",
       "742     يتميز\n",
       "743       يجب\n",
       "744      يجعل\n",
       "745     يحتاج\n",
       "746      يحتو\n",
       "747      يريد\n",
       "748     يعتبر\n",
       "749      يعرف\n",
       "750       يعط\n",
       "751      يعمل\n",
       "752       يعن\n",
       "753      يقدم\n",
       "754       يقع\n",
       "755      يقول\n",
       "756      يقوم\n",
       "757       يكن\n",
       "758      يكون\n",
       "759       يمك\n",
       "760       ينم\n",
       "761       يهم\n",
       "762      يوجد\n",
       "763      يورو\n",
       "764      يوفر\n",
       "765       يوم\n",
       "766     يومين\n",
       "Name: Word, Length: 767, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show vocabulary words that have SMALLEST IDF values (i.e. that have the largest document frequencies)\n",
    "df.sort_values(\"IDF\", inplace=False, ascending = True).head(10)\n",
    "df['Word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.593543</td>\n",
       "      <td>اجد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.593543</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>5.593543</td>\n",
       "      <td>مفيد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>5.593543</td>\n",
       "      <td>جو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>5.593543</td>\n",
       "      <td>استاهل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>5.593543</td>\n",
       "      <td>مرور</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>5.593543</td>\n",
       "      <td>فرع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>5.593543</td>\n",
       "      <td>يوفر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>5.593543</td>\n",
       "      <td>تعرف</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>5.593543</td>\n",
       "      <td>اطعم</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF    Word\n",
       "16   5.593543     اجد\n",
       "8    5.593543      50\n",
       "629  5.593543    مفيد\n",
       "234  5.593543      جو\n",
       "42   5.593543  استاهل\n",
       "594  5.593543    مرور\n",
       "472  5.593543     فرع\n",
       "764  5.593543    يوفر\n",
       "194  5.593543    تعرف\n",
       "68   5.593543    اطعم"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show vocabulary words that have LARGEST IDF values (i.e. that have the smallest document frequencies)\n",
    "df.sort_values(\"IDF\", inplace=False, ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want, you can save the vocabulary into a file\n",
    "# df.sort_values(\"IDF\", inplace=False, ascending = True).to_csv(\"./models/bow_vocab.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove documents that do not contain any vocabulary terms\n",
    "i.e. remove rows in the DTM that are all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10773, 767), (10773,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_terms_per_doc = np.array((ara_bow_dtm>0).sum(axis=1))  # calculate sum of rows of DTM matrix\n",
    "nb_terms_per_doc = nb_terms_per_doc.ravel()  # convert result to a 1D array (instead of 2D array)\n",
    "ara_bow_dtm.shape,nb_terms_per_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10773.000000\n",
       "mean        24.224728\n",
       "std         21.419477\n",
       "min          0.000000\n",
       "25%          8.000000\n",
       "50%         19.000000\n",
       "75%         33.000000\n",
       "max        241.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(nb_terms_per_doc).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2e0befa25f8>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(nb_terms_per_doc).plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2e0c05a8c50>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(nb_terms_per_doc[nb_terms_per_doc<100]).plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1962"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of documents that contain LESS THAN 5 vocabulary term\n",
    "np.sum(nb_terms_per_doc<=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of documents that contain ZERO vocabulary terms\n",
    "np.sum(nb_terms_per_doc==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ايجابيا و سلبيا قطاع ثوم مبتكر',\n",
       " 'سامسونج جالكس اس دوال S7562 اسود شريحت',\n",
       " 'رب يحفظ مارشال يمن دوس مجرم انقلاب',\n",
       " 'دام عز ك طنا',\n",
       " 'هنجم راس',\n",
       " 'هب',\n",
       " 'معجب',\n",
       " 'خام زعيم الفذ',\n",
       " 'دستور',\n",
       " 'مبرو الف مبر',\n",
       " 'موبايل جامد اوي رستيش',\n",
       " 'اعز لله يبطل لام عربيه اسلام',\n",
       " 'حمد لله',\n",
       " 'ايجابيا و سلبيا توكنج توم القط متكلم GP2130',\n",
       " 'ووه',\n",
       " 'ايجابيا و سلبيا خاخ لعصر ليم',\n",
       " 'ممتااز',\n",
       " 'رفع راس يابطال ع ايد حناء',\n",
       " 'راايع',\n",
       " 'وعد اوفي',\n",
       " 'قرارا تثلج صدر',\n",
       " 'صناع سامسونج',\n",
       " 'ممتااز',\n",
       " 'عشت يابطل',\n",
       " 'ربنا يبار فيك ياريس',\n",
       " 'وايد حلوو واحب',\n",
       " 'ودر كولاج',\n",
       " 'اكرم',\n",
       " 'يمن',\n",
       " 'ايجابيا و سلبيا انجر تنكر قطو',\n",
       " 'ايجابيا و سلبيا طقم نيو واتش SW5005 White نساء',\n",
       " 'ايجابيا و سلبيا سامسونج جالكس I9105 اس2 بلس 8 جيجاباي اندرويد كحل غامق',\n",
       " '85',\n",
       " 'شراب كولاج امريك',\n",
       " 'حفظ اله ياصقر',\n",
       " 'ربنا يكرم',\n",
       " 'قرار حكيم ايه زعيم',\n",
       " 'رووع',\n",
       " 'زعيم طراز نادر',\n",
       " 'عجب واحبب',\n",
       " 'سكو ياهاد يدل ع حنك',\n",
       " 'زعيم متواضع وحدو',\n",
       " 'تقيمي متواضع',\n",
       " 'االل معك زعيم',\n",
       " 'راايع',\n",
       " 'حبيبي مرسي',\n",
       " 'راجل',\n",
       " 'ممتازز',\n",
       " '7عغ',\n",
       " 'حميل',\n",
       " 'نعم فيك',\n",
       " 'ابداع',\n",
       " 'حلوو عجب مرر',\n",
       " 'حن',\n",
       " 'امين',\n",
       " 'طاريا تدوم لزم اطول شاح',\n",
       " 'ربنا يعين ريس و ينصر',\n",
       " 'عيت تشتغل وش سالفه',\n",
       " 'كلش عجب اشو مكالمه مصير رجو منكم اسول ارسال فيد',\n",
       " 'ابو حمص شومش اخت واخ تحديثا ماضل زاكر جوال',\n",
       " 'زفت يعلق لعق كلب اجرب',\n",
       " 'اووف ليش مابشتغل علا سامسونج نوت 4',\n",
       " 'لابد اقال',\n",
       " 'عميل',\n",
       " 'ارحم',\n",
       " 'يلع ابو',\n",
       " 'حتود داه',\n",
       " 'ياخروف',\n",
       " 'ايد ذال',\n",
       " 'قرار غب',\n",
       " 'يمن',\n",
       " 'طز',\n",
       " 'كسم',\n",
       " 'قرف',\n",
       " 'ملعون',\n",
       " 'خاين',\n",
       " 'حمار',\n",
       " 'اتحمل مسيول واستقيل وامش ريح',\n",
       " 'يوال يهود يسمح امريك يقتل مواط ايش تتوقع من يسك مجرم',\n",
       " '',\n",
       " 'تسقط مرسي',\n",
       " 'غب',\n",
       " 'اسك توتو',\n",
       " 'ارحل يافاشل',\n",
       " 'وين جا بهيمه',\n",
       " '',\n",
       " 'باطل',\n",
       " 'متبطل متاجر بقي باسم شهداء كفا ضلال',\n",
       " 'خراط',\n",
       " 'خرب بلاد عباد',\n",
       " 'الالالالالالال دستور',\n",
       " 'تبا حقير تطمىء رعي دماج ترسل الجيش يقتل',\n",
       " 'موافق',\n",
       " 'معترض',\n",
       " 'دمنا رخص جوه وبر',\n",
       " 'طظ بيك يامرس',\n",
       " 'كذاب',\n",
       " 'هيحرق بلاد',\n",
       " 'ينفعش كده',\n",
       " 'قرار موفق',\n",
       " 'اخرف',\n",
       " 'فشل ابن فشل',\n",
       " 'كسم',\n",
       " 'باطل',\n",
       " 'منكم لله',\n",
       " 'انعل ابو مرشد كلب',\n",
       " 'ارحل منافق',\n",
       " 'طظ',\n",
       " 'فاكر انفس اذكياء',\n",
       " 'عبيط',\n",
       " 'حقير',\n",
       " 'منافق',\n",
       " 'يسقط مرس مبار',\n",
       " 'نتيج ايه ارحم غلا',\n",
       " 'ياريس نعم تعبان',\n",
       " 'انتبه طابور خامس زراع فتن شعوب',\n",
       " 'لال',\n",
       " 'طظ',\n",
       " 'منافق افاق ورح تاكل خري امريك صحاب اسراييل',\n",
       " 'خلاص فين احك ولل تعب',\n",
       " 'قبح عميل',\n",
       " 'نريد قرارا صارم لمنع تكرار حوادث مستقبلء',\n",
       " 'كفا نهض',\n",
       " 'ارحم شوي',\n",
       " 'ارحم',\n",
       " 'دستور مرس',\n",
       " 'احلق دقن عار تلقي وشك وش مبار',\n",
       " 'افعل شيت ترحل جيت',\n",
       " 'كذاب',\n",
       " 'يلع ابو اللي جاب لاد كلب جزم',\n",
       " 'طرطور',\n",
       " 'خبر فاض',\n",
       " 'ارحل جبان',\n",
       " 'يسقط حكم مرشد خروف',\n",
       " 'عاش خاين يمو خاين لعنه ياطرطور',\n",
       " 'ارحل اصلع',\n",
       " 'غلط',\n",
       " 'مسكين مايعرف يتكلم',\n",
       " 'ربنا يتنقم منك',\n",
       " 'حكوم فاشل فاشل',\n",
       " 'مرسي تفوق ترحل',\n",
       " 'سعرو زياد و فاهم ايس كوف غلط',\n",
       " 'نيفس تهف و تعك',\n",
       " 'زق و ابو كلب',\n",
       " 'لقي صرصور و اظفر',\n",
       " 'قصد دار شهداء دار امو دار مقابر',\n",
       " 'لحد يجي شال يجيب مرض',\n",
       " 'ستاهل هالشهر',\n",
       " 'اسغلال اغلي باريس لند جنيف',\n",
       " 'ماتشتغل',\n",
       " 'شكرل',\n",
       " 'يناسب مع',\n",
       " 'استلم',\n",
       " 'مصنوع خاما تعبان',\n",
       " 'يشح جوال',\n",
       " 'سىء',\n",
       " 'اانصح اقتناء',\n",
       " 'ممتازةوش',\n",
       " 'الو تظهر اعل مطلق',\n",
       " 'تصلح غرض صنع لاجل',\n",
       " 'اتصلح اسعمال جوال',\n",
       " 'مضروب',\n",
       " 'مواد مطابق لصور',\n",
       " 'اا',\n",
       " 'اانصح',\n",
       " 'رووع',\n",
       " 'ايجابيا و سلبيا انجر موديل 030 احمر',\n",
       " 'شكرا',\n",
       " 'ايجابيا و سلبيا تابل ترابيز لاب توب',\n",
       " 'اباس',\n",
       " 'ايجابيا و سلبيا مشد تنحيف جسم']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which documents have length 0? (i.e. contain 0 words)\n",
    "[doc for i,doc in enumerate(ara_corpus_bow) if nb_terms_per_doc[i]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = nb_terms_per_doc>0\n",
    "ara_bow_dtm_filt = ara_bow_dtm[nb_terms_per_doc>0,:]\n",
    "ara_corpus_bow_filt = [ara_corpus_bow[i] for i,x in enumerate(idx) if x]\n",
    "ara_corpus_sentiment_filt = [ara_corpus_sentiment[i] for i,x in enumerate(idx) if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10773, 767), 10773)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ara_bow_dtm.shape,len(ara_corpus_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10602, 767), 10602, 10602)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ara_bow_dtm_filt.shape,len(ara_corpus_bow_filt),len(ara_corpus_sentiment_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 2\n",
    "\n",
    "Answer the following questions based on visual inspection of the above results:\n",
    "\n",
    "1. How many words are then in the BOW vocabulary and how many words were ignored?\n",
    "2. Give examples of 5 words in the vocabulary set of BOW but are not very useful for machine learning.\n",
    "3. Give examples of 5 words that are **not** in the voabulary of BOW but might be useful.\n",
    "4. Give examples of 5 words in the vocabulary set of BOW that are useful but should be stemmed or normalized.\n",
    "5. Propose new values of the configuration parameters of BOW in order to tune the vocabulary to be more satisfactory. \n",
    "\n",
    "**Important Remarks**:\n",
    "- Your answer to all questions will depend on how you do **text cleaning**. So you should rectify your answers after completing text cleaning part (at home).\n",
    "- Your answer to Q4 will be quite **subjective**, so you just need to justify your choice. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "\n",
    "1) there are 767 words in bow vocabulary and 29298 are ignored\n",
    "\n",
    "2) These are 5 words in the vocabulary set of BOW but are not very useful for machine learning.\n",
    "[\n",
    ",اتصال\n",
    ",استخدام\n",
    ",اسعار\n",
    ",برنامج\n",
    ",جهاز\n",
    "]\n",
    "\n",
    "3)These are 5 words that are not in the voabulary of BOW but might be useful.\\n\n",
    "[\n",
    ",عيوب\n",
    ",متننمق\n",
    ",فااشل\n",
    ",مشاء\n",
    ",عنيف\n",
    "]\n",
    "\n",
    "4)an example of a word in the vocabulary set of BOW that is useful but should be stemmed or normalized\n",
    "is يومين which should be يوم \n",
    "\n",
    "5)maybe \n",
    "\n",
    "maxdf = 9.0\n",
    "mindf = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Sentiment Classifier\n",
    "We will use the machine learning approach, i.e. train a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = ara_bow_dtm_filt\n",
    "y = ara_corpus_sentiment_filt\n",
    "y[670:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=1996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7421, 767)\n",
      "7421\n",
      "(3181, 767)\n",
      "3181\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(len(y_train))\n",
    "print(X_test.shape)\n",
    "print(len(y_test))\n",
    "assert(len(y_test)+len(y_train)==len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read documentation\n",
    "# ?LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 383)\t0.804834816524051\n",
      "  (0, 549)\t0.5934988779354998\n",
      "  (1, 18)\t0.1334486128769071\n",
      "  (1, 49)\t0.12261955000137052\n",
      "  (1, 69)\t0.10511510418424344\n",
      "  (1, 102)\t0.13897345032599753\n",
      "  (1, 117)\t0.16043377643497803\n",
      "  (1, 124)\t0.17150514773404718\n",
      "  (1, 128)\t0.15629699968163174\n",
      "  (1, 134)\t0.20958048954367142\n",
      "  (1, 140)\t0.1057769231341755\n",
      "  (1, 196)\t0.12979811711701975\n",
      "  (1, 214)\t0.1117007130740321\n",
      "  (1, 227)\t0.14122203209349413\n",
      "  (1, 242)\t0.1430904689566383\n",
      "  (1, 248)\t0.38730406065256157\n",
      "  (1, 259)\t0.12910135355085386\n",
      "  (1, 298)\t0.11555020604281023\n",
      "  (1, 305)\t0.06407686482946066\n",
      "  (1, 329)\t0.19477022137307187\n",
      "  (1, 360)\t0.1088151676034125\n",
      "  (1, 367)\t0.13497518580503592\n",
      "  (1, 395)\t0.09271454363446954\n",
      "  (1, 428)\t0.44053734889832286\n",
      "  (1, 481)\t0.1468231967088472\n",
      "  :\t:\n",
      "  (7419, 512)\t0.07553041182270966\n",
      "  (7419, 515)\t0.07057530003858054\n",
      "  (7419, 518)\t0.052479563634621355\n",
      "  (7419, 520)\t0.09113706056485625\n",
      "  (7419, 522)\t0.0750334845488266\n",
      "  (7419, 530)\t0.055691717545269324\n",
      "  (7419, 548)\t0.07756445951157813\n",
      "  (7419, 550)\t0.07394315081207584\n",
      "  (7419, 554)\t0.07604239977000621\n",
      "  (7419, 568)\t0.0634955085477602\n",
      "  (7419, 592)\t0.07513170755389371\n",
      "  (7419, 628)\t0.08801457513938295\n",
      "  (7419, 700)\t0.08286648143749041\n",
      "  (7419, 701)\t0.05173586710055048\n",
      "  (7419, 708)\t0.16149471513106017\n",
      "  (7419, 725)\t0.17710189493606973\n",
      "  (7419, 727)\t0.07291893970095796\n",
      "  (7419, 729)\t0.0758871676584212\n",
      "  (7419, 732)\t0.06700152957616386\n",
      "  (7419, 750)\t0.09315870970148092\n",
      "  (7419, 751)\t0.14317991059386315\n",
      "  (7419, 756)\t0.07802576624064496\n",
      "  (7420, 229)\t0.5010008662184428\n",
      "  (7420, 233)\t0.7764503371806499\n",
      "  (7420, 305)\t0.3822603902321363\n",
      "[-1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, -1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, -1, 1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, -1, 1, -1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, -1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, 1, -1, -1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, -1, -1, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, -1, 1, -1, -1, 1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, 1, 1, 1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, -1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, 1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, 1, -1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, 1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, -1, 1, -1, -1, 1, -1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, -1, -1, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, -1, -1, -1, -1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1, -1, -1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, -1, -1, -1, 1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, -1, 1, -1, -1, 1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, -1, 1, 1, 1, -1, -1, 1, -1, 1, -1, -1, 1, -1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, -1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, -1, 1, 1, 1, -1, -1, 1, -1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, -1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, -1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, 1, 1, 1, -1, -1, -1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, -1, -1, 1, 1, 1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, 1, 1, 1, -1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, -1, 1, -1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, -1, -1, -1, -1, 1, -1, 1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1, -1, -1, 1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1, 1, -1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, -1, -1, -1, 1, -1, -1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1, 1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, 1, -1, -1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, -1, -1, 1, -1, -1, -1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, -1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Train the model using Logistic Regression method\n",
    "\n",
    "LR_model = LogisticRegression(penalty='l2')\n",
    "LR_model.fit(X_train, y_train)\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this model to predict the sentiment category of test documents\n",
    "y_pred_LR = LR_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, 3181)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_pred_LR),len(y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8745677459918265"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the classification rate of this classifier\n",
    "metrics.accuracy_score(y_test, y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1216  210]\n",
      " [ 189 1566]]\n"
     ]
    }
   ],
   "source": [
    "# Display the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_pred_LR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret parameter values of  this model\n",
    "- The logistic regression model has one parameter per feature (i.e. vocabulary word).\n",
    "- Most positive values indicate parameters that contribute most to class 1\n",
    "- Most negative values indicate parameters contribute most to class -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-train the model using ALL DATA\n",
    "LR_model2 = LogisticRegression(penalty='l2')\n",
    "LR_model2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(767,)\n"
     ]
    }
   ],
   "source": [
    "# get the coefficients (parameter) of the LR model\n",
    "LR_coefs = LR_model2.coef_   #2D array with only one row\n",
    "LR_coefs = LR_coefs.ravel()  #convert to a 1D array\n",
    "print(type(LR_coefs))\n",
    "print(LR_coefs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the number of coefficients = number of words in vocabulary\n",
    "assert(len(bow_model.get_feature_names())==len(LR_coefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    767.000000\n",
       "mean       0.146287\n",
       "std        1.139649\n",
       "min       -8.825373\n",
       "25%       -0.450909\n",
       "50%        0.139757\n",
       "75%        0.698925\n",
       "max        8.542629\n",
       "dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the coefficients in a Pandas Series for ease of visualization\n",
    "pd.Series(LR_coefs).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2e0c0689828>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(LR_coefs).plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Coef   Word\n",
      "351  2.490052     سن\n",
      "643  2.507385   مميز\n",
      "702  2.520219   واسع\n",
      "20   2.548937    احب\n",
      "21   2.574019   احبب\n",
      "538  2.623451   لطيف\n",
      "329  2.693935   زيار\n",
      "74   2.784734   اعجب\n",
      "302  2.793359    راق\n",
      "537  2.810146   لذيذ\n",
      "38   2.920442   اروع\n",
      "19   2.970736   اجمل\n",
      "641  3.009011   ممتع\n",
      "378  3.065609    شكر\n",
      "595  3.525490   مريح\n",
      "319  3.536763    روع\n",
      "237  3.878377    جيد\n",
      "230  4.610472   جميل\n",
      "640  6.423544  ممتاز\n",
      "305  8.542629   رايع\n",
      "         Coef   Word\n",
      "358 -8.825373    سيء\n",
      "58  -5.085689   اسوء\n",
      "496 -4.110045    قذر\n",
      "9   -4.099727    ابد\n",
      "607 -3.105039     مش\n",
      "56  -3.028851    اسف\n",
      "313 -2.650020   رديء\n",
      "423 -2.615222    عاد\n",
      "331 -2.594887     سء\n",
      "338 -2.512282    سبب\n",
      "400 -2.325473   ضعيف\n",
      "460 -2.284838    غال\n",
      "287 -2.268465    دفع\n",
      "656 -2.246257     مو\n",
      "488 -2.231787    قال\n",
      "70  -2.182266  اطلاق\n",
      "556 -2.162182  مبالغ\n",
      "243 -2.151523    حجز\n",
      "202 -2.043724    تكن\n",
      "495 -1.956303   قديم\n"
     ]
    }
   ],
   "source": [
    "## COMPLETE THE CODE BELOW BASED ON THE INSTRUCTIONS IN THE COMMENTS.\n",
    "# HINT: put the coefficients and the vocabulary words in a Pandas DataFrame ...\n",
    "\n",
    "\n",
    "# 1) Sort coefficient values in ascending order\n",
    "df = pd.DataFrame(dict(Word=bow_vocab, Coef=LR_coefs))\n",
    "\n",
    "df_sorted=df.sort_values(\"Coef\", inplace=False, ascending = True)\n",
    "# 2) Display the 20 largest coefficients and their corresponding words\n",
    "print(df_sorted.tail(20))\n",
    "\n",
    "# 3) Display the 20 smallest coefficients and their corresponding words\n",
    "\n",
    "print(df_sorted.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 3\n",
    "\n",
    "Answer the following questions based on the results of the logistic regression model.\n",
    "\n",
    "1. Based on the *confusion matrix* above, is this classifier **biased**?  Note: we say a classifier is *biased* if it makes *significantly* more errors for one class than the other (false positives vs. false negatives).\n",
    "2. Which 5 words are most aossicatd with positive sentiment?  (hint: see most positive coefficients)\n",
    "3. Which 5 words are most aossicatd with negative sentiment?  (hint: see most negative coefficients)\n",
    "4. Do these results make sense?  (if they don't, then there is something wrong with the data and/or processing pipeline ...)\n",
    "5. (OPTIONAL) Show all test documents that are positive but are predicted negative by this classifier. Do you notice any trends?\n",
    "6. (OPTIONAL) Show all test documents that are negative but are predicted positive by this classifier. Do you notice any trends?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "1) Yes based on the confusion matrix this model is biased.\n",
    "\n",
    "2)\n",
    "سن\n",
    "مميز\n",
    "واسع\n",
    "احب\n",
    "احبب\n",
    "\n",
    "3)\n",
    "سيء\n",
    "اسوء\n",
    "قذر\n",
    "ابد\n",
    "مش\n",
    "\n",
    "4)yes, these results make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read documentation\n",
    "# ?MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build Naive Bayes classification model\n",
    "\n",
    "NB_model = MultinomialNB(alpha = 1.0)\n",
    "NB_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this model to predict the sentiment category of test documents\n",
    "y_pred_NB = NB_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8478465891229173"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification rate\n",
    "metrics.accuracy_score(y_test, y_pred_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1121,  305],\n",
       "       [ 179, 1576]], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret parameters of  the model\n",
    "- Naive Bayes model has two parameters per feature (i.e. vocabulary word)\n",
    "- These parameters are stored in a 2x$n$ array, where $n$ is the number of vocabulary words\n",
    "- The first row of this array contains **log of conditional probabilities** Pr(xi|class1)\n",
    "- The second row contains **log of conditional probabilities** Pr(xi|class2)\n",
    "- Ideally, if parameter value $\\approx$ 0 then the corresponding vocabulary word is very important for that class\n",
    "- Ideally, if parameter value $\\approx - \\infty$ then the corresponding vocabulary word is not important at all for that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-train the model using ALL DATA\n",
    "NB_model2 = MultinomialNB(alpha = 1.0)\n",
    "NB_model2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(2, 767)\n"
     ]
    }
   ],
   "source": [
    "# get NB model's coefficients\n",
    "NB_coefs = NB_model2.feature_log_prob_\n",
    "print(type(NB_coefs))\n",
    "print(NB_coefs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(NB_coefs.shape[0] == 2)\n",
    "assert(NB_coefs.shape[1] == len(bow_model.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the order of classes in this model\n",
    "NB_model2.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramaters associated with the NEGATIVE class\n",
    "NB_coefs_neg_class = NB_coefs[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramaters associated with the POSITIVE class\n",
    "NB_coefs_pos_class = NB_coefs[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    767.000000\n",
       "mean      -6.928116\n",
       "std        0.723262\n",
       "min       -9.221461\n",
       "25%       -7.409733\n",
       "50%       -6.981655\n",
       "75%       -6.497663\n",
       "max       -4.207456\n",
       "dtype: float64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(NB_coefs_neg_class).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2e0c0758400>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(NB_coefs_neg_class).plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2e0c0667860>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(NB_coefs_pos_class).plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Coef   Word\n",
      "117 -5.365797     ان\n",
      "687 -5.365008   نظيف\n",
      "82  -5.328791   افضل\n",
      "486 -5.304400     في\n",
      "618 -5.292318   مطعم\n",
      "453 -5.292163    عند\n",
      "124 -5.287880   انصح\n",
      "634 -5.269805   مكان\n",
      "661 -5.258978   موقع\n",
      "229 -5.154877   جميع\n",
      "268 -5.000449    خدم\n",
      "674 -4.950977    ندق\n",
      "462 -4.844801    غرف\n",
      "230 -4.680282   جميل\n",
      "237 -4.564261    جيد\n",
      "640 -4.511203  ممتاز\n",
      "480 -4.487416   فندق\n",
      "515 -4.411526    كان\n",
      "221 -4.314320    جدا\n",
      "305 -4.191665   رايع\n",
      "         Coef   Word\n",
      "496 -9.325855    قذر\n",
      "313 -9.045912   رديء\n",
      "355 -8.909264    سوء\n",
      "58  -8.758174   اسوء\n",
      "11  -8.716512   اتصل\n",
      "331 -8.709919     سء\n",
      "416 -8.664309    طلع\n",
      "335 -8.529750    سال\n",
      "26  -8.528116   اخبر\n",
      "246 -8.395234    حدث\n",
      "672 -8.370201    نجم\n",
      "241 -8.331896   حاول\n",
      "48  -8.323018  استطع\n",
      "232 -8.322888    جني\n",
      "387 -8.299094    صار\n",
      "639 -8.248438   مليء\n",
      "309 -8.231526    رحت\n",
      "533 -8.200100   لدرج\n",
      "178 -8.183535   تحدث\n",
      "440 -8.174080   عشان\n",
      "         Coef  Word\n",
      "762 -5.440843  يوجد\n",
      "415 -5.428558   طلب\n",
      "660 -5.427144  موظف\n",
      "486 -5.417970    في\n",
      "342 -5.414721   سعر\n",
      "118 -5.385368   انا\n",
      "674 -5.371078   ندق\n",
      "453 -5.362702   عند\n",
      "237 -5.340434   جيد\n",
      "618 -5.337685  مطعم\n",
      "94  -5.216856   اكل\n",
      "268 -5.190985   خدم\n",
      "607 -5.177580    مش\n",
      "128 -5.174109   انه\n",
      "117 -5.084615    ان\n",
      "462 -4.652962   غرف\n",
      "480 -4.609259  فندق\n",
      "221 -4.561497   جدا\n",
      "358 -4.386802   سيء\n",
      "515 -4.207456   كان\n",
      "         Coef     Word\n",
      "38  -9.221461     اروع\n",
      "21  -9.092358     احبب\n",
      "585 -8.919428     مذهل\n",
      "563 -8.892510    متميز\n",
      "60  -8.862928     اشكر\n",
      "711 -8.666412    وانصح\n",
      "74  -8.580098     اعجب\n",
      "688 -8.517829     نعود\n",
      "697 -8.506558     هدوء\n",
      "47  -8.489493  استرخاء\n",
      "52  -8.425215  استمتاع\n",
      "647 -8.421148    مناظر\n",
      "764 -8.399006     يوفر\n",
      "25  -8.377591     احمد\n",
      "574 -8.370235     مجهز\n",
      "184 -8.356580    ترحيب\n",
      "234 -8.318415       جو\n",
      "354 -8.307173     سهول\n",
      "641 -8.302430     ممتع\n",
      "487 -8.261877     قادم\n"
     ]
    }
   ],
   "source": [
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "# Sort the coefficient values of the POSITIVE CLASS in ascending order\n",
    "\n",
    "df_pos = pd.DataFrame(dict(Word=bow_vocab, Coef=NB_coefs_pos_class))\n",
    "\n",
    "df_pos_sorted=df_pos.sort_values(\"Coef\", inplace=False, ascending = True)\n",
    "\n",
    "# Display the 20 largest coefficients and their corresponding words\n",
    "\n",
    "print(df_pos_sorted.tail(20))\n",
    "\n",
    "# Display the 20 smallest coefficients and their corresponding words\n",
    "\n",
    "print(df_pos_sorted.head(20))\n",
    "\n",
    "\n",
    "\n",
    "# Repeat the same thing for the coefficients of the negative class\n",
    "\n",
    "\n",
    "df_neg = pd.DataFrame(dict(Word=bow_vocab, Coef=NB_coefs_neg_class))\n",
    "\n",
    "df_neg_sorted=df_neg.sort_values(\"Coef\", inplace=False, ascending = True)\n",
    "\n",
    "# Display the 20 largest coefficients and their corresponding words\n",
    "\n",
    "print(df_neg_sorted.tail(20))\n",
    "\n",
    "# Display the 20 smallest coefficients and their corresponding words\n",
    "\n",
    "print(df_neg_sorted.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 4\n",
    "\n",
    "Answer the following questions based on the results of the Naive Bayes model.\n",
    "\n",
    "1. Based on the *confusion matrix* above, is this Naive Bayes classifier **biased**?  Note: we say a classifier is *biased* if it makes significantly more errors for one class than the other (false positives vs. false negatives).\n",
    "2. Which classification method is more accurate for this data: \n",
    "2. Which 5 words are most aossicatd with positive sentiment?  (hint: see most positive coefficients)\n",
    "3. Which 5 words are most aossicatd with negative sentiment?  (hint: see most negative coefficients)\n",
    "4. Do these results make sense?  (if they don't, then there is something wrong with the data and/or processing pipeline ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "\n",
    "1)Yes, the Naive Bayes classifier is biased.\n",
    "\n",
    "2)The first method is more accurate.\n",
    "\n",
    "3)\n",
    "ان\n",
    "نظيف\n",
    "افضل\n",
    "في\n",
    "مطعم\n",
    "\n",
    "4)\n",
    "يوجد\n",
    "طلب\n",
    "موظف\n",
    "في\n",
    "سعر\n",
    "انا\n",
    "\n",
    "5)These results are not very convincing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 5 (OPTIONAL)\n",
    "Repeat thE entire TD for TUN corpus.  This is optional for the TD, but you need to do it anyway for the final project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tun_corpus_files = [ './sentiment_data_TUN_pos.txt', './sentiment_data_TUN_neg.txt' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
